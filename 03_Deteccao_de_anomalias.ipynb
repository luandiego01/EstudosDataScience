{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## __Exercício: Detecção de Anomalias__\n",
    "\n",
    "<br>\n",
    "\n",
    "__1:__\n",
    "\n",
    "Utilizando a classe DetectorAnomalias criada abaixo, __vamos avaliar um detector de anomalias.__\n",
    "\n",
    "O dataset utilizado pode ser importado através da função getData. \n",
    "\n",
    "Nesse conjunto de dados, possuímos 6 variáveis explicativas, $X_1, .., X_6$ e uma variável com a marcação se a instância é uma anomalia ou não.\n",
    "\n",
    "Utilizando a __metodolodia__ discutida ao longo do módulo, __teste diferentes modelos (variando o limiar $\\epsilon$)__ a fim de encontrar o que __melhor fita os dados.__\n",
    "\n",
    "Justifique as escolhas do $\\epsilon$, bem como quais as métricas de performance abordadas. \n",
    "\n",
    "<br>\n",
    "\n",
    "__2:__ \n",
    "\n",
    "Aborde o problema num contexto de aprendizado supervisionado, ou seja, treine modelos de classificação binária com o objetivo de detectar anomalias.\n",
    "\n",
    "Compare os resultados entre as metodologias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorAnomalias():\n",
    "    \n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def fit(self, X):\n",
    "        medias = X.mean(axis = 0)\n",
    "        desvios = X.std(axis = 0)\n",
    "        gaussianas = [st.norm(loc = m, scale = d) for m, d in zip(medias, desvios)]  \n",
    "        self.gaussianas = gaussianas\n",
    "        self.X = X\n",
    "        \n",
    "    def prob(self, x):\n",
    "        p = 1\n",
    "        for i in range(self.X.shape[1]):\n",
    "            gaussiana_i = self.gaussianas[i]\n",
    "            x_i = x[i]\n",
    "            p *= gaussiana_i.pdf(x_i)\n",
    "        return p\n",
    "    \n",
    "    def isAnomaly(self, x):\n",
    "        return int(np.where(self.prob(x) < self.epsilon, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    return pd.read_csv(\"dataframe_anomalias_exercicio.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>anomalia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.731153</td>\n",
       "      <td>23.299155</td>\n",
       "      <td>-0.367453</td>\n",
       "      <td>4.715372</td>\n",
       "      <td>9.306179</td>\n",
       "      <td>16.780965</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.466833</td>\n",
       "      <td>16.943695</td>\n",
       "      <td>-0.245131</td>\n",
       "      <td>7.060311</td>\n",
       "      <td>10.462826</td>\n",
       "      <td>19.821289</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.501272</td>\n",
       "      <td>20.196011</td>\n",
       "      <td>1.206049</td>\n",
       "      <td>-4.957189</td>\n",
       "      <td>7.771262</td>\n",
       "      <td>19.100079</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.893921</td>\n",
       "      <td>16.072385</td>\n",
       "      <td>2.738045</td>\n",
       "      <td>-3.684228</td>\n",
       "      <td>7.373334</td>\n",
       "      <td>23.225524</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.091706</td>\n",
       "      <td>19.253894</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>-9.504052</td>\n",
       "      <td>8.883988</td>\n",
       "      <td>17.903298</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10095</th>\n",
       "      <td>11.192286</td>\n",
       "      <td>18.451987</td>\n",
       "      <td>-0.953650</td>\n",
       "      <td>-14.362996</td>\n",
       "      <td>10.875826</td>\n",
       "      <td>17.056541</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10096</th>\n",
       "      <td>12.014177</td>\n",
       "      <td>19.461815</td>\n",
       "      <td>1.985099</td>\n",
       "      <td>-7.119190</td>\n",
       "      <td>11.079922</td>\n",
       "      <td>17.582755</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10097</th>\n",
       "      <td>10.745460</td>\n",
       "      <td>18.175951</td>\n",
       "      <td>0.206037</td>\n",
       "      <td>-1.897015</td>\n",
       "      <td>9.888329</td>\n",
       "      <td>17.963324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10098</th>\n",
       "      <td>9.893969</td>\n",
       "      <td>22.333270</td>\n",
       "      <td>-1.465981</td>\n",
       "      <td>4.137382</td>\n",
       "      <td>7.690620</td>\n",
       "      <td>21.570097</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10099</th>\n",
       "      <td>11.563246</td>\n",
       "      <td>20.777121</td>\n",
       "      <td>-1.145317</td>\n",
       "      <td>-4.650515</td>\n",
       "      <td>11.857622</td>\n",
       "      <td>19.424881</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x1         x2        x3         x4         x5         x6  \\\n",
       "0       7.731153  23.299155 -0.367453   4.715372   9.306179  16.780965   \n",
       "1      11.466833  16.943695 -0.245131   7.060311  10.462826  19.821289   \n",
       "2      11.501272  20.196011  1.206049  -4.957189   7.771262  19.100079   \n",
       "3      10.893921  16.072385  2.738045  -3.684228   7.373334  23.225524   \n",
       "4      10.091706  19.253894  0.996895  -9.504052   8.883988  17.903298   \n",
       "...          ...        ...       ...        ...        ...        ...   \n",
       "10095  11.192286  18.451987 -0.953650 -14.362996  10.875826  17.056541   \n",
       "10096  12.014177  19.461815  1.985099  -7.119190  11.079922  17.582755   \n",
       "10097  10.745460  18.175951  0.206037  -1.897015   9.888329  17.963324   \n",
       "10098   9.893969  22.333270 -1.465981   4.137382   7.690620  21.570097   \n",
       "10099  11.563246  20.777121 -1.145317  -4.650515  11.857622  19.424881   \n",
       "\n",
       "       anomalia  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "10095       0.0  \n",
       "10096       0.0  \n",
       "10097       0.0  \n",
       "10098       0.0  \n",
       "10099       0.0  \n",
       "\n",
       "[10100 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = getData()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    10046\n",
       "1.0       54\n",
       "Name: anomalia, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.anomalia.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso objetivo é encontrar o melhor limiar para a classe DetectorAnomalias criada acima, para isso iremos seguir um passo a passo, sendo eles:\n",
    "\n",
    "__1 - Dividir nosso dataset em três datasets da seguinte forma:__\n",
    "\n",
    "_Dataset de treino com 6000 linhas sem nenhuma anomalia._\n",
    "\n",
    "_Dataset de validação com 2050 linhas, sendo 27 delas anomalias_\n",
    "\n",
    "_Dataset de treino com 2050 linhas, sendo 27 delas também anomalias_\n",
    "\n",
    "__2 - Encontrar a média e desvio padrão de todas as colunas do dataset de treino.__\n",
    "\n",
    "__3 - Fazer um loop para encontrar a menor gaussiana de todos os registros também no dataset de treino.__\n",
    "\n",
    "\n",
    "Dessa forma como sabemos que não tem nenhuma anomalia no dataset de treino, podemos escolher o valor encontrado no passo 3 como o limiar para o nosso objeto da classe DetectorAnomalias que criaremos.\n",
    "\n",
    "Depois disso, podemos comparar esse limiar, com outros limiares da mesma ordem para ver qual deles se sair melhor.\n",
    "\n",
    "Após encontrar esses limiares, testaremos nosso Detector no dataset de validação e de teste, e por fim, utilizaremos a F1 Score para medir a precisão que temos com os limiares escolhidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando as anomalias das não anomalias\n",
    "\n",
    "dfano0 = df[df['anomalia']!=1]\n",
    "dfano1 = df[df['anomalia']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10046, 7), (54, 7))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfano0.shape, dfano1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando os datasets de treino, validacao e teste\n",
    "\n",
    "dftreino = dfano0.iloc[:6000,:]\n",
    "dfvalidacao = pd.concat([dfano0.iloc[6000:8023,:],dfano1.iloc[:27,:]],axis = 0)\n",
    "dfteste =  pd.concat([dfano0.iloc[8023:,:],dfano1.iloc[27:,:]],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6000, 7), (2050, 7), (2050, 7))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftreino.shape, dfvalidacao.shape, dfteste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos que ter cuidado, pois nosso dataset tem uma coluna \"anomália\" que não é uma variável explicativa, portanto, vamos criar novos dados de treino, validação e teste, sem a última coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6000, 6), (2050, 6), (2050, 6))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtreino = dftreino.values[:,:-1]\n",
    "\n",
    "Xvalid = dfvalidacao.values[:,:-1]\n",
    "\n",
    "Xteste = dfteste.values[:,:-1]\n",
    "\n",
    "Xtreino.shape, Xvalid.shape, Xteste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluimos então o primeiro passo, agora vamos calcular a média e desvio padrão das colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediaXtreino = Xtreino.mean(axis = 0)\n",
    "desvioXtreino = Xtreino.std(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora criaremos um código onde vamos calcular a função:\n",
    "\n",
    "$p(x) = p(x_1)\\cdot p(x_2) \\cdot p(x_3) \\cdot p(x_4) \\cdot p(x_5) \\cdot p(x_6)$, onde cada $p(x_i)$ é a densidade segundo a gaussiana.\n",
    "\n",
    "Vamos calcular em todas as linhas do dataset de treino e printar o menor valor obtido, assim concluímos o passo 3 e temos um condidato para ser nosso limiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00000000674761196991165743619185910244680226544034\n"
     ]
    }
   ],
   "source": [
    "listagaussiana = []\n",
    "\n",
    "for n in range(0,Xtreino.shape[0]): \n",
    "    p = 1\n",
    "    gaussianas = [st.norm(loc = m, scale = d) for m, d in zip(mediaXtreino, desvioXtreino)]\n",
    "    for i in range(Xtreino.shape[1]):\n",
    "        gaussiana_i = gaussianas[i]\n",
    "        x_i = Xtreino[n,][i]\n",
    "        p*= gaussiana_i.pdf(x_i)\n",
    "    listagaussiana.append(p)\n",
    "\n",
    "print('{:.50f}'.format(min(listagaussiana)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtemos então nosso limiar, vamos construir nosso objeto para detectar anomalias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DetectorAnomalias(0.00000000674761196991165743619185910244680226544034)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitando nos dados de treino\n",
    "\n",
    "dtc.fit(Xtreino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conferir, vamos criar uma lista com todas as previsões no próprio dataset de treino, e printar o maior valor, já que não temos anomálias no dataset de treino, esperamos que o valor mostrado seja 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "listatreino = []\n",
    "\n",
    "p = 1\n",
    "for i in range(Xtreino.shape[0]):\n",
    "    listatreino.append(dtc.isAnomaly(Xtreino[i, ]))\n",
    "\n",
    "print(max((listatreino)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faremos a mesma coisa na lista de validação e de teste, mas dessa vez printaremos a soma dos valores, o melhor valor esperado é de 27 para cada um dos datasets, vamos ver o que conseguimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "listavalid = []\n",
    "\n",
    "p = 1\n",
    "for i in range(Xvalid.shape[0]):\n",
    "    listavalid.append(dtc.isAnomaly(Xvalid[i, ]))\n",
    "\n",
    "print(sum((listavalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "listateste = []\n",
    "\n",
    "p = 1\n",
    "for i in range(Xteste.shape[0]):\n",
    "    listateste.append(dtc.isAnomaly(Xteste[i, ]))\n",
    "\n",
    "print(sum((listateste)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não conseguimos uma precisão de 100%, mas chegamos perto, conseguindo obter quase todas as anomalias, tanto nos dados de validação, Vamos calcular o F1 score neles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredvalid = np.array(listavalid).reshape(-1,1)\n",
    "yvalid = dfvalidacao['anomalia'].values.reshape(-1,1)\n",
    "\n",
    "ypredteste = np.array(listateste).reshape(-1,1)\n",
    "yteste = dfteste['anomalia'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9811320754716981"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1Scorevalid = f1_score(y_true = yvalid, y_pred = ypredvalid)\n",
    "\n",
    "f1Scorevalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9411764705882353"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1Scoreteste = f1_score(y_true = yteste, y_pred = ypredteste)\n",
    "\n",
    "f1Scoreteste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como imaginavamos, os 98% e 94% concidem com as 26 e 24 anomalias encontradas nos dados de validação e de teste, respectivamente.\n",
    "\n",
    "Nosso limiar é da cada de $10^{-9}$, vamos então variar um limiar nessa mesma ordem pra ver se conseguimos algum outro limiar com resultado parecido com o obtido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anomalias detectadas para Limiar:  5e-09\n",
      "\n",
      "Dados de treino:  0\n",
      "\n",
      "Dados de validação:  25\n",
      "\n",
      "Dados de teste:  16\n",
      "\n",
      "Anomalias detectadas para Limiar:  6e-09\n",
      "\n",
      "Dados de treino:  0\n",
      "\n",
      "Dados de validação:  25\n",
      "\n",
      "Dados de teste:  23\n",
      "\n",
      "Anomalias detectadas para Limiar:  7e-09\n",
      "\n",
      "Dados de treino:  1\n",
      "\n",
      "Dados de validação:  27\n",
      "\n",
      "Dados de teste:  25\n",
      "\n",
      "Anomalias detectadas para Limiar:  7.999999999999999e-09\n",
      "\n",
      "Dados de treino:  2\n",
      "\n",
      "Dados de validação:  27\n",
      "\n",
      "Dados de teste:  28\n",
      "\n",
      "Anomalias detectadas para Limiar:  9e-09\n",
      "\n",
      "Dados de treino:  4\n",
      "\n",
      "Dados de validação:  28\n",
      "\n",
      "Dados de teste:  29\n"
     ]
    }
   ],
   "source": [
    "for limiar in np.arange(0.000000005,0.00000001, 0.000000001):\n",
    "  \n",
    "    dc = DetectorAnomalias(limiar)\n",
    "    dc.fit(Xtreino)\n",
    "    listatreino = []\n",
    "    listavalid = []\n",
    "    listateste = []\n",
    "\n",
    "    p = 1\n",
    "    for i in range(Xtreino.shape[0]):\n",
    "        listatreino.append(dc.isAnomaly(Xtreino[i, ]))\n",
    "\n",
    "\n",
    "    p = 1\n",
    "    for i in range(Xvalid.shape[0]):\n",
    "        listavalid.append(dc.isAnomaly(Xvalid[i, ]))\n",
    "\n",
    "\n",
    "    p = 1\n",
    "    for i in range(Xteste.shape[0]):\n",
    "        listateste.append(dc.isAnomaly(Xteste[i, ]))\n",
    "\n",
    "\n",
    "    print('\\nAnomalias detectadas para Limiar: ', limiar)\n",
    "    print('\\nDados de treino: ', sum((listatreino)))\n",
    "    print('\\nDados de validação: ', sum((listavalid)))\n",
    "    print('\\nDados de teste: ', sum((listateste)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conseguimos outro limiar também muito bom, $7\\cdot 10^{-9}$, que apesar de detectar um elemento dos dados de treino como anomalia, conseguiu encontrar todas as anomalias do dados de validação e quase todas nos dados de teste, vamos calcular o F1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc1 = DetectorAnomalias(0.000000007)\n",
    "\n",
    "dc1.fit(Xtreino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9615384615384615)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listavalid1 = []\n",
    "\n",
    "p = 1\n",
    "for i in range(Xvalid.shape[0]):\n",
    "    listavalid1.append(dc1.isAnomaly(Xvalid[i, ]))\n",
    "\n",
    "listateste1 = []\n",
    "\n",
    "p = 1\n",
    "for i in range(Xteste.shape[0]):\n",
    "    listateste1.append(dc1.isAnomaly(Xteste[i, ]))\n",
    "    \n",
    "ypredvalid1 = np.array(listavalid1).reshape(-1,1)\n",
    "\n",
    "ypredteste1 = np.array(listateste1).reshape(-1,1)\n",
    "    \n",
    "    \n",
    "f1Scorevalid1 = f1_score(y_true = yvalid, y_pred = ypredvalid1)\n",
    "\n",
    "f1Scoreteste1 = f1_score(y_true = yteste, y_pred = ypredteste1)\n",
    "\n",
    "f1Scorevalid1, f1Scoreteste1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dois F1 Score nos diz que realmente todos as anomalias detectadas são as anomalias reais, sem nenhum falso positivo.\n",
    "\n",
    "Terminamos então o Exercício 1 com dois possíveis Limiares com ótimos resultados:\n",
    "\n",
    "__0.0000000067 e 0.000000007__\n",
    "\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2:__ \n",
    "\n",
    "Aborde o problema num contexto de aprendizado supervisionado, ou seja, treine modelos de classificação binária com o objetivo de detectar anomalias.\n",
    "\n",
    "Compare os resultados entre as metodologias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________\n",
    "\n",
    "#### Lembrando como é nossa tabela original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>anomalia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.731153</td>\n",
       "      <td>23.299155</td>\n",
       "      <td>-0.367453</td>\n",
       "      <td>4.715372</td>\n",
       "      <td>9.306179</td>\n",
       "      <td>16.780965</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.466833</td>\n",
       "      <td>16.943695</td>\n",
       "      <td>-0.245131</td>\n",
       "      <td>7.060311</td>\n",
       "      <td>10.462826</td>\n",
       "      <td>19.821289</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.501272</td>\n",
       "      <td>20.196011</td>\n",
       "      <td>1.206049</td>\n",
       "      <td>-4.957189</td>\n",
       "      <td>7.771262</td>\n",
       "      <td>19.100079</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.893921</td>\n",
       "      <td>16.072385</td>\n",
       "      <td>2.738045</td>\n",
       "      <td>-3.684228</td>\n",
       "      <td>7.373334</td>\n",
       "      <td>23.225524</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.091706</td>\n",
       "      <td>19.253894</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>-9.504052</td>\n",
       "      <td>8.883988</td>\n",
       "      <td>17.903298</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10095</th>\n",
       "      <td>11.192286</td>\n",
       "      <td>18.451987</td>\n",
       "      <td>-0.953650</td>\n",
       "      <td>-14.362996</td>\n",
       "      <td>10.875826</td>\n",
       "      <td>17.056541</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10096</th>\n",
       "      <td>12.014177</td>\n",
       "      <td>19.461815</td>\n",
       "      <td>1.985099</td>\n",
       "      <td>-7.119190</td>\n",
       "      <td>11.079922</td>\n",
       "      <td>17.582755</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10097</th>\n",
       "      <td>10.745460</td>\n",
       "      <td>18.175951</td>\n",
       "      <td>0.206037</td>\n",
       "      <td>-1.897015</td>\n",
       "      <td>9.888329</td>\n",
       "      <td>17.963324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10098</th>\n",
       "      <td>9.893969</td>\n",
       "      <td>22.333270</td>\n",
       "      <td>-1.465981</td>\n",
       "      <td>4.137382</td>\n",
       "      <td>7.690620</td>\n",
       "      <td>21.570097</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10099</th>\n",
       "      <td>11.563246</td>\n",
       "      <td>20.777121</td>\n",
       "      <td>-1.145317</td>\n",
       "      <td>-4.650515</td>\n",
       "      <td>11.857622</td>\n",
       "      <td>19.424881</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x1         x2        x3         x4         x5         x6  \\\n",
       "0       7.731153  23.299155 -0.367453   4.715372   9.306179  16.780965   \n",
       "1      11.466833  16.943695 -0.245131   7.060311  10.462826  19.821289   \n",
       "2      11.501272  20.196011  1.206049  -4.957189   7.771262  19.100079   \n",
       "3      10.893921  16.072385  2.738045  -3.684228   7.373334  23.225524   \n",
       "4      10.091706  19.253894  0.996895  -9.504052   8.883988  17.903298   \n",
       "...          ...        ...       ...        ...        ...        ...   \n",
       "10095  11.192286  18.451987 -0.953650 -14.362996  10.875826  17.056541   \n",
       "10096  12.014177  19.461815  1.985099  -7.119190  11.079922  17.582755   \n",
       "10097  10.745460  18.175951  0.206037  -1.897015   9.888329  17.963324   \n",
       "10098   9.893969  22.333270 -1.465981   4.137382   7.690620  21.570097   \n",
       "10099  11.563246  20.777121 -1.145317  -4.650515  11.857622  19.424881   \n",
       "\n",
       "       anomalia  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "10095       0.0  \n",
       "10096       0.0  \n",
       "10097       0.0  \n",
       "10098       0.0  \n",
       "10099       0.0  \n",
       "\n",
       "[10100 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos com um problema de classificação, vamos usar dois modelos de Machine Learning, o KNN e o Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftreino, dfteste = train_test_split(df, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar fazendo uma normalização, com o auxílio da mesma função de pré-processamento usada nos outros módulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessamento_completo(df, dataset_de_treino = True, std_scaler = None):\n",
    "\n",
    "    dff = df.copy()\n",
    "    \n",
    "    variaveis_para_normalizar = ['x1',\n",
    "                                 'x2',\n",
    "                                'x3',\n",
    "                                 'x4',\n",
    "                                 'x5',\n",
    "                                 'x6',\n",
    "                                 ]\n",
    "\n",
    "    if dataset_de_treino:  \n",
    "\n",
    "        sc = StandardScaler()\n",
    "        variaveis_norm = sc.fit_transform(dff[variaveis_para_normalizar])\n",
    "        \n",
    "        X, y =  variaveis_norm, dff['anomalia'].values\n",
    "        return X, y, sc\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        variaveis_norm = std_scaler.transform(dff[variaveis_para_normalizar]) \n",
    "        \n",
    "        X, y =  variaveis_norm, dff['anomalia'].values\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtreino, ytreino, stan = preprocessamento_completo(df = dftreino, dataset_de_treino=True, std_scaler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xteste, yteste = preprocessamento_completo(df =dfteste, dataset_de_treino=False, std_scaler=stan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7575, 6), (7575,), (2525, 6), (2525,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtreino.shape, ytreino.shape, Xteste.shape, yteste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer a validação cruzada manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 em treino: \n",
      " [1.0, 1.0, 1.0, 1.0, 1.0]  \n",
      "| média:  1.0\n",
      "\n",
      "F1 em validação: \n",
      " [0.3636363636363636, 0.4, 0.0, 0.0, 0.0]  \n",
      "| média:  0.15272727272727274\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=20)\n",
    "\n",
    "lista_f1_treino = []\n",
    "lista_f1_validacao = []\n",
    "\n",
    "for train_index, val_index in kf.split(Xtreino, ytreino):\n",
    "    \n",
    "    Xtrain_folds = Xtreino[train_index]\n",
    "    ytrain_folds = ytreino[train_index]\n",
    "    Xval_fold = Xtreino[val_index]\n",
    "    yval_fold = ytreino[val_index]\n",
    "    \n",
    "    rf.fit(Xtrain_folds, ytrain_folds)\n",
    "    \n",
    "    pred_treino = rf.predict(Xtrain_folds)\n",
    "    pred_validacao = rf.predict(Xval_fold)\n",
    "    \n",
    "    lista_f1_treino.append(f1_score(y_pred = pred_treino, y_true = ytrain_folds))\n",
    "    lista_f1_validacao.append(f1_score(y_pred = pred_validacao, y_true = yval_fold))\n",
    "    \n",
    "    \n",
    "print(\"F1 em treino: \\n\", lista_f1_treino, \" \\n| média: \", np.mean(lista_f1_treino))\n",
    "print()\n",
    "print(\"F1 em validação: \\n\", lista_f1_validacao, \" \\n| média: \", np.mean(lista_f1_validacao))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já conseguimos perceber o que ta acontecendo, temos uma quantidade muito baixa de anomalias, o que deixa muito dificil a previsão correta, vamos agora testar com o KNN, e depois aplicar os dois modelos nos dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 em treino: \n",
      " [0.25, 0.1621621621621622, 0.1875, 0.30303030303030304, 0.1875]  \n",
      "| média:  0.21803849303849304\n",
      "\n",
      "F1 em validação: \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
      "| média:  0.0\n"
     ]
    }
   ],
   "source": [
    "kn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "lista_f1_treino = []\n",
    "lista_f1_validacao = []\n",
    "\n",
    "for train_index, val_index in kf.split(Xtreino, ytreino):\n",
    "    \n",
    "    Xtrain_folds = Xtreino[train_index]\n",
    "    ytrain_folds = ytreino[train_index]\n",
    "    Xval_fold = Xtreino[val_index]\n",
    "    yval_fold = ytreino[val_index]\n",
    "    \n",
    "    kn.fit(Xtrain_folds, ytrain_folds)\n",
    "    \n",
    "    pred_treino = kn.predict(Xtrain_folds)\n",
    "    pred_validacao = kn.predict(Xval_fold)\n",
    "    \n",
    "    lista_f1_treino.append(f1_score(y_pred = pred_treino, y_true = ytrain_folds))\n",
    "    lista_f1_validacao.append(f1_score(y_pred = pred_validacao, y_true = yval_fold))\n",
    "    \n",
    "    \n",
    "print(\"F1 em treino: \\n\", lista_f1_treino, \" \\n| média: \", np.mean(lista_f1_treino))\n",
    "print()\n",
    "print(\"F1 em validação: \\n\", lista_f1_validacao, \" \\n| média: \", np.mean(lista_f1_validacao))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como também era de se esperar, modelos de classificação para tratar anomalias não boas opções, para finalizar a comparação, vamos calcular o F1 Score nos dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19047619047619047"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1RF = f1_score(y_pred = rf.predict(Xteste), y_true = yteste)\n",
    "\n",
    "f1RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1KNN = f1_score(y_pred = kn.predict(Xteste), y_true = yteste)\n",
    "\n",
    "f1KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como já comentado, temos uma quantidade muito pequena de anomalias para que modelos de classificação consigam aprender bem sobre eles, por exemplo, se nos dados de treino não tiver nenhuma anomalia, como o modelo irá aprender a achar alguma? Ou Seja, para esse caso, o contexto não supervisionado é o melhor para se trabalhar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
